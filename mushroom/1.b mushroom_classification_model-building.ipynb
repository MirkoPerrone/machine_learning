{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 5pt>Building a binary classifier for the mushroom dataset</font><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neccesary packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prep the data\n",
    "df= pd.read_csv('data/cleaned_mushroom.csv')\n",
    "\n",
    "df0 = df.copy()\n",
    "df0.head()\n",
    "df0.drop('Unnamed: 0', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names= ['cap_shape', 'cap_surface', 'cap_color', 'bruise?', 'odor',\n",
    "       'gill attachment', 'gill spacing', 'gill size', 'gill color',\n",
    "       'stalk shape', 'stalk root', 'stalk-surface-above-ring',\n",
    "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "       ' stalk-color-below-ring', 'veil-type', 'veil color', 'ring number',\n",
    "       'ring type', 'spore-print-color', 'population', 'habitat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform transformation and label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in col_names:\n",
    "    df0[i]= label_encoder.fit_transform(df0[i])\n",
    "\n",
    "# poisonous: 1, edible= 0\n",
    "df0['class']= np.where(df0['class'] == 'poisonous', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables for each categorical feature..\n",
    "df_dum= pd.get_dummies(df0.iloc[:, 1:], columns= col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df0.iloc[:, 1:]\n",
    "y = df0.iloc[:, 0]\n",
    "\n",
    "X_dum = df_dum.values\n",
    "y_dum = df0.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'logit test score': 0.9266}\n",
      "{'logit train score': 0.9184}\n",
      "\n",
      " {'decision tree test score': 1.0}\n",
      "{'decision tree train score': 1.0}\n",
      "\n",
      " {'random forest test score': 0.9902}\n",
      "{'random forest train score': 0.9915}\n",
      "\n",
      " {'naive bayes test score': 0.836}\n",
      "{'naive bayes train score': 0.8285}\n"
     ]
    }
   ],
   "source": [
    "# fit various models with default parameters for (no dummy)\n",
    "logit = LogisticRegression(C= 0.01)\n",
    "tree = DecisionTreeClassifier(max_depth=8, max_features=5)\n",
    "forest  = RandomForestClassifier(max_features=5, max_depth=5, n_estimators=10)\n",
    "mNB = MultinomialNB(alpha= 10)          # high alpha means moore smoothing and less complex models.\n",
    "\n",
    "names = ['logit', 'decision tree', 'random forest', 'naive bayes']\n",
    "models = [logit, tree, forest, mNB]\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_scores = {\n",
    "        name+' test score': np.round(model.score(X_test, y_test),4)\n",
    "    }\n",
    "    train_scores = {\n",
    "        name+' train score': np.round(model.score(X_train, y_train),4)\n",
    "    }\n",
    "    print('\\n',test_scores)\n",
    "    print(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'steelblue'> Note</font><br>\n",
    "We are interested in a model that does well on the training data and does even better in the test data, thus we want a model that has less chance of overfiting the data but good in generalizing.\n",
    "Here **Naive Bayes Algorithm** performs badly among the rest, but might do better in generalzing.\n",
    "\n",
    "**Logistics Regression** and **Random Forest** model might be our good to go model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiting the model with dummy data\n",
    "X_train1, X_test1, y_train1, y_test1= train_test_split(X_dum, y_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'logit test score': 0.9838}\n",
      "{'logit train score': 0.9856}\n",
      "\n",
      " {'decision tree test score': 0.9724}\n",
      "{'decision tree train score': 0.9744}\n",
      "\n",
      " {'random forest test score': 0.9828}\n",
      "{'random forest train score': 0.9828}\n",
      "\n",
      " {'naive bayes test score': 0.9355}\n",
      "{'naive bayes train score': 0.9329}\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(C= 0.01) # searching for optimal regularization. high C means less regularization\n",
    "tree = DecisionTreeClassifier(max_depth=8, max_features=5)# pre-prunning model.\n",
    "forest  = RandomForestClassifier(max_features=5, max_depth=5, n_estimators=10)# pre-prunning model.\n",
    "mNB = MultinomialNB(alpha= 10)                    # high alpha means moore smoothing and less complex models.\n",
    "\n",
    "names = ['logit', 'decision tree', 'random forest', 'naive bayes']\n",
    "models = [logit, tree, forest, mNB]\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    model.fit(X_train1, y_train1)\n",
    "    test_scores = {\n",
    "        name+' test score': np.round(model.score(X_test1, y_test1),4)\n",
    "    }\n",
    "    train_scores = {\n",
    "        name+' train score': np.round(model.score(X_train1, y_train1), 4)\n",
    "    }\n",
    "    print('\\n',test_scores)\n",
    "    print(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'><b>Note</b></font><br>\n",
    "The Naive Bayes algorithms even does better after transforming the data. I think is a sure go to model for prediciton.\n",
    "So, finally we choose the Naive Bayes algorithm to predict new instances, It has less chance overfitting the data and have a predicition score on the test set. **93%** accuracy is pretty good."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   ## next\n",
    "1. feature selection (to reduce the number of feature but generalize well) using tree algorithms\n",
    "1. building a pipeline with onehot encoding on very categorical column\n",
    "2. metrics binary classification. (confusion matrix, roc, auc)\n",
    "3. saving the model\n",
    "4. building a simple web app"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
